{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from disk\n",
      "Done!                            \n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "from dmsensei import DataModule, create_model, Dataset\n",
    "from dmsensei.config import device\n",
    "from dmsensei.core.callbacks import WandbFitLogger, KaggleLogger\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "from lightning.pytorch import Trainer\n",
    "from dmsensei.config import device\n",
    "import sys\n",
    "import os\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import torch, wandb\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "dm = DataModule(\n",
    "    name=[\"ribo-valid\"],#, 'sarah_supermodel'],\n",
    "    data_type=[\"dms\", 'structure'],\n",
    "    force_download=False,\n",
    "    batch_size=32,\n",
    "    num_workers=0,\n",
    "    train_split=256,\n",
    "    valid_split=0,\n",
    "    predict_split=0,\n",
    "    overfit_mode=True,\n",
    "    shuffle_valid=False,\n",
    "    use_error=False,\n",
    ")\n",
    "\n",
    "dm.setup('fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, tensor\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "seq2int = {'X': 0, 'A': 1, 'C': 2, 'G': 3, 'U': 4, 'S': 5, 'E': 6}\n",
    "START_TOKEN = seq2int['S']\n",
    "END_TOKEN = seq2int['E']\n",
    "\n",
    "def load_ct(path):\n",
    "    return pd.read_csv(path, sep=' ', header=None, names=['i', 'j', 'p'])\n",
    "\n",
    "# reduce weight initialization variance\n",
    "import torch.nn.init as init\n",
    "\n",
    "# Set the global scale for weight initialization\n",
    "global_scale = 0.1\n",
    "    \n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params\n",
    "    ):\n",
    "        \n",
    "        super().__init__()  \n",
    "        self.conv = nn.Conv2d(params['num_heads'], params['num_heads'], 3, padding=1)\n",
    "        self.batch2d = nn.BatchNorm2d(7)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.gammas = nn.Parameter(torch.ones(params['num_heads']))\n",
    "        \n",
    "    def forward(self, structure):\n",
    "        x = self.conv(structure)\n",
    "        x = self.batch2d(x)\n",
    "        x = self.gelu(x)\n",
    "        x = x + structure\n",
    "        x = x * self.gammas\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "    ):\n",
    "        super().__init__()  \n",
    "        self.layer_norm = nn.LayerNorm(params['embed_dim'])\n",
    "        self.linear1 = nn.Linear(params['embed_dim'], params['hidden_dim'])\n",
    "        self.gelu = nn.GELU()\n",
    "        self.linear2 = nn.Linear(params['hidden_dim'], params['embed_dim'])\n",
    "        \n",
    "    def forward(self, sequence):\n",
    "        self.layer_norm(sequence)\n",
    "        sequence = self.linear1(sequence)\n",
    "        sequence = self.gelu(sequence)\n",
    "        sequence = self.linear2(sequence)\n",
    "        return sequence\n",
    "        \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "    ):\n",
    "        super().__init__()  \n",
    "        self.self_attention = SelfAttention(params)\n",
    "        self.feed_forward = FeedForward(params)\n",
    "        \n",
    "    def forward(self, sequence, structure):\n",
    "        encoded_sequence, encoded_structure = self.self_attention(sequence, structure)\n",
    "        sequence = sequence + encoded_sequence\n",
    "        sequence = sequence + self.feed_forward(sequence)\n",
    "        return sequence, encoded_structure\n",
    "    \n",
    "\n",
    "class Ribonanza(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "    ):\n",
    "        super().__init__()  \n",
    "        self.params = params\n",
    "        self.ntokens = 7\n",
    "        self.table_embedding = nn.Embedding(self.ntokens, params['embed_dim'])\n",
    "        self.table_embedding.weight.data.normal_(mean=0.0, std=0.2)\n",
    "        self.output_net = nn.Linear(params['embed_dim'], 2)\n",
    "        params['table_embedding'] = self.table_embedding\n",
    "        self.encoders_stack = nn.ModuleList([Encoder(params) for _ in range(params['num_encoders'])])\n",
    "        # Initialize the weights with a reduced scale\n",
    "        init.xavier_uniform_(self.table_embedding.weight, gain=global_scale)\n",
    "        init.xavier_uniform_(self.output_net.weight, gain=global_scale)\n",
    "\n",
    "    def forward(self, sequence, structure, padding=None):\n",
    "        if padding is None:\n",
    "            if type(sequence) == str:\n",
    "                sequence = [sequence]\n",
    "                structure = [structure]\n",
    "            padding = max([len(seq) for seq in sequence])\n",
    "        sequence = torch.stack([self.embed_sequence(seq, padding) for seq in sequence])\n",
    "        structure = torch.stack([self.embed_structure(struct, padding) for struct in structure])\n",
    "        for encoder in self.encoders_stack:\n",
    "           # print(sequence[0,0,0])\n",
    "            sequence, structure = encoder(sequence, structure)\n",
    "        x = self.output_net(sequence)\n",
    "        return x\n",
    "    \n",
    "    def embed_structure(self, idx, dim):\n",
    "        matrix = torch.zeros((dim+2, dim+2))\n",
    "        matrix[idx[:,0], idx[:,1]] = 1\n",
    "        matrix = matrix.repeat(self.params['num_heads'], 1, 1)\n",
    "        return matrix.reshape(self.params['num_heads'], dim+2, dim+2)\n",
    "    \n",
    "    def embed_sequence(self, sequence, padding=None):\n",
    "        # add tokens for start and end of sequence\n",
    "        sequence = 'S' + sequence + 'E' \n",
    "        if padding is not None:\n",
    "            sequence = sequence + 'X' * (padding - len(sequence))\n",
    "        \n",
    "        # convert to one-hot\n",
    "        sequence = tensor([seq2int[s] for s in sequence])\n",
    "        \n",
    "        # convert to embedding\n",
    "        sequence = self.table_embedding(sequence)\n",
    "        \n",
    "        return sequence.reshape(-1, sequence.shape[1])\n",
    "    \n",
    "    \n",
    "class ConvSE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params\n",
    "    ):\n",
    "        super().__init__()  \n",
    "        self.conv = nn.Conv2d(params['num_heads'], params['num_heads'], 3, padding=1)\n",
    "        self.batch2d = nn.BatchNorm2d(params['num_heads'])\n",
    "        self.gelu = nn.GELU()   \n",
    "        \n",
    "        # Squeeze and Excitation\n",
    "        self.adaptive_average_pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(params['num_heads'], params['num_heads'])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(params['num_heads'], params['num_heads'])\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, structure):\n",
    "        x = self.conv(structure)\n",
    "        x = self.batch2d(x)\n",
    "        # SE\n",
    "        y = self.adaptive_average_pooling(x)\n",
    "        y = y.view(-1, params['num_heads']) # unsure\n",
    "        y = self.fc1(y)\n",
    "        y = self.relu(y)\n",
    "        y = self.fc2(y)\n",
    "        y = self.sigmoid(y)\n",
    "        y = y.view(-1, params['num_heads'], 1, 1) # unsure \n",
    "        x = x * y\n",
    "        # end SE\n",
    "        x = self.gelu(x)\n",
    "        x = x + structure\n",
    "        return x\n",
    "    \n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "    ):\n",
    "        super().__init__()  \n",
    "        self.convSE = ConvSE(params)\n",
    "        self.pos_encoding = DynamicPositionalEncoding(params)\n",
    "        \n",
    "    def forward(self, sequence, structure):\n",
    "        # self attention\n",
    "        # sequence = sequence.permute(1, 0, 2) # check if this is correct\n",
    "        structure = self.convSE(structure)\n",
    "        sequence = sequence.reshape(sequence.shape[0], sequence.shape[1], params['num_heads'], params['dim_per_head']).permute(0, 2, 1, 3)\n",
    "        attention = torch.matmul(sequence, sequence.permute(0, 1, 3, 2))\n",
    "        attention =  attention + structure + self.pos_encoding(sequence)\n",
    "        attention = attention / attention.sum(dim=2, keepdim=True) # unsure about this\n",
    "        attention = attention @ sequence\n",
    "        # end self attention\n",
    "        attention = torch.reshape(attention, (attention.shape[0], attention.shape[2], -1))\n",
    "        return attention, structure\n",
    "    \n",
    "    \n",
    "class DynamicPositionalEncoding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "    ):\n",
    "        super().__init__()  \n",
    "        self.params = params\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(params['max_len'], params['max_len'], 1))\n",
    "        self.lin1 = nn.Linear(1, 48)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.lin2 = nn.Linear(48, 48)\n",
    "        self.lin3 = nn.Linear(48, params['num_heads'])\n",
    "        # init.xavier_uniform_(self.lin1, gain=global_scale)\n",
    "        # init.xavier_uniform_(self.lin2, gain=global_scale)\n",
    "        # init.xavier_uniform_(self.lin3, gain=global_scale)\n",
    "    \n",
    "    def forward(self, sequence):\n",
    "        # will this work with batches?\n",
    "        seq_len = sequence.shape[2]\n",
    "        x = self.positional_encoding[:seq_len, :seq_len, :]\n",
    "        x = self.lin1(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.lin3(x)\n",
    "        return x.permute(2, 0, 1).reshape(1, -1, seq_len, seq_len)\n",
    "\n",
    "params = {\n",
    "    'embed_dim': 192,\n",
    "    'num_heads': 6,\n",
    "    'hidden_dim': 768,\n",
    "    'num_encoders': 12,\n",
    "    'max_len': 430,\n",
    "}\n",
    "params['dim_per_head'] = params['embed_dim'] // params['num_heads']\n",
    "\n",
    "model = Ribonanza(params)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.MSELoss()\n",
    "for batch in dm.train_dataloader():\n",
    "    sequence = batch.sequence\n",
    "    structure = batch.structure\n",
    "    pred = model.forward(sequence, structure, padding=None)[:, :, 0]\n",
    "    pairs = [(p, t) for p, t in zip(pred, batch.dms) if t is not None]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.tensor(0.)\n",
    "    for p, t in pairs:\n",
    "        p = p[1:-1]\n",
    "        mask = t != -1000.\n",
    "        if not torch.sum(mask):\n",
    "            continue\n",
    "        p, t = p[mask], t[mask]\n",
    "        loss += criterion(p, t)\n",
    "    loss.backward()\n",
    "    \n",
    "\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
