{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push the model's prediction to Kaggle\n",
    "\n",
    "### Load the model and the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/yvesmartin/.pyenv/versions/3.11.6/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightning.pytorch import Trainer\n",
    "import os, sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "from dmsensei.core import DataModule\n",
    "\n",
    "# model = pickle.load(open('model.pkl','rb'))\n",
    "trainer = Trainer()\n",
    "import torch\n",
    "\n",
    "# dm = DataModule(\n",
    "#             name=[\"utr\"],\n",
    "#             force_download=False,\n",
    "#             batch_size=256,\n",
    "#             num_workers=1,\n",
    "#             train_split=40,\n",
    "#             valid_split=40,\n",
    "#             predict_split=1.,\n",
    "#             overfit_mode=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using local data for: ribonanza\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wrangling data for ribonanza: 49001it [00:02, 22555.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using local data for: ribonanza\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wrangling data for ribonanza: 49001it [00:02, 23906.79it/s]\n",
      "/Users/yvesmartin/src/DMSensei/dmsensei/models/multi/transformer.py:113: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(m.weight * 0.001)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Embedding(5, 64)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): Sequential(\n",
       "    (0): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.0, inplace=False)\n",
       "      (dropout2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.0, inplace=False)\n",
       "      (dropout2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.0, inplace=False)\n",
       "      (dropout2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.0, inplace=False)\n",
       "      (dropout2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.0, inplace=False)\n",
       "      (dropout2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.0, inplace=False)\n",
       "      (dropout2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.0, inplace=False)\n",
       "      (dropout2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.0, inplace=False)\n",
       "      (dropout2): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (resnet): Sequential(\n",
       "    (0): ResLayer(\n",
       "      (res_blocks): Sequential(\n",
       "        (0): ResBlock(\n",
       "          (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(32, 32), dilation=(32, 32), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), bias=False)\n",
       "        )\n",
       "        (2): ResBlock(\n",
       "          (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(16, 16), dilation=(16, 16), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(8, 8), dilation=(4, 4), bias=False)\n",
       "        )\n",
       "        (3): ResBlock(\n",
       "          (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (conv_output): Conv2d(1, 8, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    )\n",
       "    (1): ResLayer(\n",
       "      (res_blocks): Sequential(\n",
       "        (0): ResBlock(\n",
       "          (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(32, 32), dilation=(32, 32), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), bias=False)\n",
       "        )\n",
       "        (2): ResBlock(\n",
       "          (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(16, 16), dilation=(16, 16), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(8, 8), dilation=(4, 4), bias=False)\n",
       "        )\n",
       "        (3): ResBlock(\n",
       "          (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8), bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (conv_output): Conv2d(8, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    )\n",
       "  )\n",
       "  (output_net_DMS): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (output_net_SHAPE): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(os.path.abspath(\".\"))\n",
    "from dmsensei import DataModule, create_model, metrics\n",
    "from dmsensei.config import device\n",
    "from dmsensei.core.callbacks import ModelChecker, MyWandbLogger, KaggleLogger\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "import pandas as pd\n",
    "from lightning.pytorch import Trainer\n",
    "from dmsensei.config import device\n",
    "import sys\n",
    "import os\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "d_model = 64\n",
    "lr = 1e-3\n",
    "gamma = 0.999\n",
    "batch_size = 4\n",
    "\n",
    "# # Create dataset\n",
    "dm = DataModule(\n",
    "    name=[\"ribonanza\"],\n",
    "    data_type=[\"dms\", \"shape\"],\n",
    "    force_download=False,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    train_split=500,\n",
    "    valid_split=250,\n",
    "    predict_split=0,\n",
    "    overfit_mode=False,\n",
    "    shuffle_valid=False,\n",
    ")\n",
    "\n",
    "model = create_model(\n",
    "    model=\"transformer\",\n",
    "    data=\"multi\",\n",
    "    ntoken=5,\n",
    "    n_struct=2,\n",
    "    d_model=d_model,\n",
    "    nhead=16,\n",
    "    d_hid=d_model,\n",
    "    nlayers=8,\n",
    "    dropout=0.0,\n",
    "    lr=lr,\n",
    "    weight_decay=0,\n",
    "    wandb=0,\n",
    "    gamma=gamma,\n",
    ")\n",
    "\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        \"/Users/yvesmartin/src/DMSensei/smooth-blaze-41.pt\",\n",
    "        map_location=torch.device(\"mps\"),\n",
    "    )\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dm.train_dataloader():\n",
    "    out = model.forward(batch.get('sequence'))\n",
    "    \n",
    "    batch.integrate_prediction(out)\n",
    "    # lod = batch.to_list_of_datapoints()\n",
    "    # dp = lod[0]\n",
    "    # dp.compute_error_metrics_pack()\n",
    "    # print(dp.metrics)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "tensor([[3, 3, 3, 1, 1, 2, 3, 1, 2, 4, 2, 3, 1, 3, 4, 1, 3, 1, 3, 4, 2, 3, 1, 1,\n",
      "         1, 1, 3, 2, 1, 4, 3, 4, 4, 3, 4, 4, 2, 3, 2, 1, 1, 1, 3, 2, 3, 1, 4, 1,\n",
      "         3, 3, 4, 4, 4, 4, 1, 4, 2, 3, 1, 2, 4, 4, 3, 2, 3, 1, 1, 4, 3, 1, 1, 4,\n",
      "         3, 2, 3, 2, 1, 2, 1, 1, 3, 4, 4, 2, 4, 3, 1, 3, 4, 3, 1, 1, 1, 4, 4, 3,\n",
      "         4, 4, 1, 4, 3, 4, 3, 4, 3, 3, 4, 3, 3, 2, 4, 3, 4, 4, 1, 4, 4, 1, 4, 3,\n",
      "         4, 4, 1, 1, 3, 2, 1, 2, 4, 2, 1, 2, 4, 1, 2, 4, 1, 1, 3, 4, 4, 2, 3, 2,\n",
      "         4, 4, 1, 3, 4, 1, 3, 4, 3, 1, 3, 4, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, 2,\n",
      "         1, 1, 2, 1, 1, 2, 1, 1, 2],\n",
      "        [3, 3, 3, 1, 1, 2, 3, 1, 2, 4, 2, 3, 1, 3, 4, 1, 3, 1, 3, 4, 2, 3, 1, 1,\n",
      "         1, 1, 3, 3, 4, 3, 2, 1, 3, 2, 3, 3, 2, 3, 4, 3, 2, 2, 3, 3, 4, 1, 3, 4,\n",
      "         2, 3, 4, 1, 2, 1, 2, 2, 4, 3, 2, 4, 3, 3, 2, 4, 2, 1, 3, 1, 1, 3, 4, 4,\n",
      "         3, 3, 4, 3, 2, 4, 3, 1, 3, 3, 3, 3, 4, 2, 3, 3, 1, 1, 1, 3, 2, 2, 3, 2,\n",
      "         3, 4, 3, 4, 4, 1, 2, 2, 3, 2, 3, 3, 2, 2, 4, 4, 4, 4, 2, 2, 3, 2, 3, 3,\n",
      "         1, 1, 4, 3, 1, 1, 3, 2, 1, 4, 2, 4, 2, 3, 2, 3, 1, 3, 2, 4, 4, 2, 3, 3,\n",
      "         4, 4, 4, 3, 2, 3, 1, 3, 3, 4, 3, 2, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, 2,\n",
      "         1, 1, 2, 1, 1, 2, 1, 1, 2],\n",
      "        [3, 3, 3, 1, 1, 2, 3, 1, 2, 4, 2, 3, 1, 3, 4, 1, 3, 1, 3, 4, 2, 3, 1, 1,\n",
      "         1, 1, 2, 2, 4, 3, 3, 3, 3, 1, 1, 3, 3, 1, 1, 2, 1, 4, 4, 2, 2, 1, 3, 3,\n",
      "         2, 1, 3, 1, 3, 2, 1, 1, 1, 4, 1, 3, 2, 1, 3, 4, 3, 2, 1, 1, 1, 3, 3, 2,\n",
      "         2, 2, 4, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 1, 2, 1, 4, 3, 2, 4, 3, 4,\n",
      "         4, 1, 3, 3, 3, 4, 1, 2, 1, 1, 3, 2, 1, 1, 4, 3, 1, 3, 3, 3, 4, 3, 3, 1,\n",
      "         3, 3, 1, 3, 4, 3, 4, 1, 2, 4, 1, 1, 2, 1, 2, 4, 4, 4, 3, 4, 4, 2, 3, 2,\n",
      "         1, 1, 1, 3, 4, 3, 4, 4, 1, 3, 4, 1, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, 2,\n",
      "         1, 1, 2, 1, 1, 2, 1, 1, 2],\n",
      "        [3, 3, 3, 1, 1, 2, 3, 1, 2, 4, 2, 3, 1, 3, 4, 1, 3, 1, 3, 4, 2, 3, 1, 1,\n",
      "         1, 1, 3, 1, 4, 1, 4, 3, 3, 1, 4, 1, 4, 4, 1, 2, 3, 1, 4, 3, 2, 4, 1, 3,\n",
      "         1, 2, 2, 2, 1, 2, 2, 3, 1, 4, 3, 1, 1, 1, 4, 2, 3, 3, 1, 1, 1, 2, 3, 1,\n",
      "         4, 3, 3, 1, 3, 3, 4, 3, 2, 4, 1, 3, 2, 4, 1, 1, 3, 4, 1, 1, 4, 1, 4, 2,\n",
      "         2, 4, 1, 1, 3, 4, 2, 3, 1, 3, 3, 4, 4, 2, 3, 3, 3, 4, 4, 2, 3, 2, 2, 2,\n",
      "         3, 1, 1, 2, 1, 1, 2, 4, 1, 4, 4, 1, 4, 3, 4, 2, 2, 1, 3, 4, 4, 2, 3, 2,\n",
      "         4, 3, 3, 1, 2, 1, 4, 1, 1, 4, 1, 3, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, 2,\n",
      "         1, 1, 2, 1, 1, 2, 1, 1, 2]], device='mps:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "print(batch.get('sequence', pred=False, true=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3693],\n",
       "         [ 0.3176],\n",
       "         [ 0.3651],\n",
       "         [ 0.6547],\n",
       "         [ 0.5760],\n",
       "         [ 0.5787],\n",
       "         [ 0.4000],\n",
       "         [ 0.3775],\n",
       "         [ 0.0877],\n",
       "         [ 0.1410],\n",
       "         [ 0.1072],\n",
       "         [ 0.2704],\n",
       "         [ 0.5729],\n",
       "         [ 0.4753],\n",
       "         [ 0.6257],\n",
       "         [ 0.6139],\n",
       "         [ 0.4680],\n",
       "         [ 0.7511],\n",
       "         [ 0.1950],\n",
       "         [ 0.3259],\n",
       "         [ 0.2909],\n",
       "         [ 0.6123],\n",
       "         [ 0.6852],\n",
       "         [ 0.6298],\n",
       "         [ 0.8307],\n",
       "         [ 0.7354],\n",
       "         [ 0.5920],\n",
       "         [ 0.6593],\n",
       "         [ 0.8995],\n",
       "         [ 0.6609],\n",
       "         [ 0.8468],\n",
       "         [ 0.7684],\n",
       "         [ 0.6234],\n",
       "         [ 0.8078],\n",
       "         [ 0.5956],\n",
       "         [ 0.4065],\n",
       "         [ 0.4824],\n",
       "         [ 0.5926],\n",
       "         [ 0.6167],\n",
       "         [ 0.7307],\n",
       "         [ 0.3287],\n",
       "         [ 0.5119],\n",
       "         [ 0.6435],\n",
       "         [ 0.5327],\n",
       "         [ 0.4505],\n",
       "         [ 0.7942],\n",
       "         [ 0.2329],\n",
       "         [ 0.3259],\n",
       "         [ 0.4819],\n",
       "         [ 0.3562],\n",
       "         [ 0.1999],\n",
       "         [ 0.1603],\n",
       "         [ 0.8001],\n",
       "         [ 0.7162],\n",
       "         [ 0.3051],\n",
       "         [ 0.6463],\n",
       "         [ 0.0720],\n",
       "         [ 0.0439],\n",
       "         [ 0.1917],\n",
       "         [ 0.2150],\n",
       "         [ 0.2300],\n",
       "         [ 0.3627],\n",
       "         [ 0.7769],\n",
       "         [ 0.7821],\n",
       "         [ 0.3747],\n",
       "         [ 0.7548],\n",
       "         [ 0.6192],\n",
       "         [ 0.2061],\n",
       "         [ 0.3027],\n",
       "         [ 0.3117],\n",
       "         [ 0.2650],\n",
       "         [ 0.2938],\n",
       "         [ 0.1470],\n",
       "         [ 0.3219],\n",
       "         [ 0.4318],\n",
       "         [ 0.7614],\n",
       "         [ 0.6277],\n",
       "         [ 0.9363],\n",
       "         [ 0.8100],\n",
       "         [ 0.7997],\n",
       "         [ 0.7241],\n",
       "         [ 0.5090],\n",
       "         [ 0.2618],\n",
       "         [ 0.2950],\n",
       "         [ 0.2320],\n",
       "         [ 0.2072],\n",
       "         [ 0.3415],\n",
       "         [ 0.4648],\n",
       "         [ 0.1255],\n",
       "         [ 0.1786],\n",
       "         [ 0.0811],\n",
       "         [ 0.1215],\n",
       "         [ 0.2930],\n",
       "         [ 0.4350],\n",
       "         [ 0.3386],\n",
       "         [ 0.5624],\n",
       "         [ 0.4748],\n",
       "         [ 0.3222],\n",
       "         [ 0.4636],\n",
       "         [ 0.2244],\n",
       "         [ 0.3170],\n",
       "         [ 0.2926],\n",
       "         [ 0.4488],\n",
       "         [ 0.3838],\n",
       "         [ 0.7076],\n",
       "         [ 0.7673],\n",
       "         [ 0.7185],\n",
       "         [ 0.5947],\n",
       "         [ 0.2421],\n",
       "         [ 0.3031],\n",
       "         [ 0.4517],\n",
       "         [ 0.3917],\n",
       "         [ 0.8527],\n",
       "         [ 0.4394],\n",
       "         [ 0.7467],\n",
       "         [ 0.2426],\n",
       "         [ 0.3670],\n",
       "         [ 0.1901],\n",
       "         [ 0.5814],\n",
       "         [ 0.5653],\n",
       "         [ 0.8337],\n",
       "         [ 0.7440],\n",
       "         [ 0.6777],\n",
       "         [ 0.7812],\n",
       "         [ 0.8833],\n",
       "         [ 0.9300],\n",
       "         [ 0.9554],\n",
       "         [ 0.8488],\n",
       "         [ 0.8109],\n",
       "         [ 0.8855],\n",
       "         [ 0.7423],\n",
       "         [ 0.8559],\n",
       "         [ 0.8177],\n",
       "         [ 0.4562],\n",
       "         [ 0.5713],\n",
       "         [ 0.5317],\n",
       "         [ 0.7518],\n",
       "         [ 0.6134],\n",
       "         [ 0.6855],\n",
       "         [ 0.4386],\n",
       "         [ 0.2492],\n",
       "         [ 0.4284],\n",
       "         [ 0.5257],\n",
       "         [ 0.4550],\n",
       "         [ 0.4689],\n",
       "         [ 0.8343],\n",
       "         [ 0.4662],\n",
       "         [ 0.3154],\n",
       "         [ 0.7022],\n",
       "         [ 0.6351],\n",
       "         [ 0.8019],\n",
       "         [ 0.5846],\n",
       "         [ 0.7175],\n",
       "         [ 0.6974],\n",
       "         [ 0.8207],\n",
       "         [ 0.7739],\n",
       "         [ 0.6570],\n",
       "         [ 0.6240],\n",
       "         [ 0.7264],\n",
       "         [ 0.7700],\n",
       "         [ 0.4555],\n",
       "         [ 0.8168],\n",
       "         [ 0.7851],\n",
       "         [ 0.7608],\n",
       "         [ 0.6671],\n",
       "         [ 0.3684],\n",
       "         [ 0.5231],\n",
       "         [ 0.2025],\n",
       "         [ 0.5234],\n",
       "         [ 0.5936],\n",
       "         [ 0.7272],\n",
       "         [ 0.5241],\n",
       "         [ 0.7757],\n",
       "         [ 0.6956],\n",
       "         [ 0.7348],\n",
       "         [ 0.7654],\n",
       "         [ 0.6392]],\n",
       "\n",
       "        [[ 0.3718],\n",
       "         [ 0.3581],\n",
       "         [ 0.4152],\n",
       "         [ 0.4199],\n",
       "         [ 0.3353],\n",
       "         [ 0.4050],\n",
       "         [ 0.2748],\n",
       "         [ 0.3481],\n",
       "         [ 0.1468],\n",
       "         [ 0.7149],\n",
       "         [ 0.2477],\n",
       "         [ 0.2883],\n",
       "         [ 0.6013],\n",
       "         [ 0.6498],\n",
       "         [ 0.8992],\n",
       "         [ 0.6883],\n",
       "         [ 0.3309],\n",
       "         [ 0.7270],\n",
       "         [ 0.3375],\n",
       "         [ 0.8288],\n",
       "         [ 0.5743],\n",
       "         [ 0.2809],\n",
       "         [ 0.6544],\n",
       "         [ 0.7862],\n",
       "         [ 0.9100],\n",
       "         [ 0.6140],\n",
       "         [ 0.2448],\n",
       "         [ 0.0892],\n",
       "         [ 0.7779],\n",
       "         [ 0.8625],\n",
       "         [ 0.8276],\n",
       "         [ 0.8649],\n",
       "         [ 0.6216],\n",
       "         [ 0.7765],\n",
       "         [ 0.7782],\n",
       "         [ 0.7792],\n",
       "         [ 0.7666],\n",
       "         [ 0.8266],\n",
       "         [ 0.7654],\n",
       "         [ 0.6549],\n",
       "         [ 0.8387],\n",
       "         [ 0.9516],\n",
       "         [ 0.8005],\n",
       "         [ 0.9191],\n",
       "         [ 0.7503],\n",
       "         [ 0.8933],\n",
       "         [ 0.6228],\n",
       "         [ 0.5192],\n",
       "         [ 0.5234],\n",
       "         [ 0.7852],\n",
       "         [ 0.8057],\n",
       "         [ 0.5997],\n",
       "         [ 0.2674],\n",
       "         [ 0.3440],\n",
       "         [ 0.3143],\n",
       "         [ 0.1645],\n",
       "         [ 0.1724],\n",
       "         [ 0.4436],\n",
       "         [ 0.1337],\n",
       "         [ 0.2576],\n",
       "         [ 0.4193],\n",
       "         [ 0.4629],\n",
       "         [ 0.1760],\n",
       "         [ 0.6638],\n",
       "         [ 0.8005],\n",
       "         [ 0.8389],\n",
       "         [ 0.7640],\n",
       "         [ 0.3725],\n",
       "         [ 0.1574],\n",
       "         [ 0.2421],\n",
       "         [ 0.5023],\n",
       "         [ 0.2493],\n",
       "         [ 0.5174],\n",
       "         [ 0.6773],\n",
       "         [ 0.8250],\n",
       "         [ 1.0576],\n",
       "         [ 0.6394],\n",
       "         [ 0.1253],\n",
       "         [ 0.3523],\n",
       "         [ 0.2891],\n",
       "         [ 0.1768],\n",
       "         [ 0.6616],\n",
       "         [ 1.0295],\n",
       "         [ 0.8271],\n",
       "         [ 0.8542],\n",
       "         [ 0.6492],\n",
       "         [ 0.9009],\n",
       "         [ 0.5040],\n",
       "         [ 0.2860],\n",
       "         [ 0.2787],\n",
       "         [ 0.0822],\n",
       "         [ 0.0971],\n",
       "         [ 0.2905],\n",
       "         [ 0.2263],\n",
       "         [ 0.2111],\n",
       "         [ 0.7022],\n",
       "         [ 0.9443],\n",
       "         [ 0.3806],\n",
       "         [ 0.3131],\n",
       "         [ 0.1525],\n",
       "         [ 0.3312],\n",
       "         [ 0.6280],\n",
       "         [ 0.6436],\n",
       "         [ 0.7120],\n",
       "         [ 0.2425],\n",
       "         [ 0.4169],\n",
       "         [ 0.2087],\n",
       "         [ 0.3806],\n",
       "         [ 0.4930],\n",
       "         [ 0.4719],\n",
       "         [ 0.5095],\n",
       "         [ 0.9045],\n",
       "         [ 0.8174],\n",
       "         [ 0.8361],\n",
       "         [ 0.6093],\n",
       "         [ 0.3326],\n",
       "         [ 0.8092],\n",
       "         [ 0.7351],\n",
       "         [ 0.5308],\n",
       "         [ 0.7164],\n",
       "         [ 0.4998],\n",
       "         [ 0.3894],\n",
       "         [ 0.6541],\n",
       "         [ 0.7345],\n",
       "         [ 0.8249],\n",
       "         [ 0.7919],\n",
       "         [ 0.6443],\n",
       "         [ 0.7526],\n",
       "         [ 0.7981],\n",
       "         [ 0.6918],\n",
       "         [ 0.6620],\n",
       "         [ 0.2071],\n",
       "         [ 0.5464],\n",
       "         [ 0.4682],\n",
       "         [ 0.6474],\n",
       "         [ 0.4258],\n",
       "         [ 0.4430],\n",
       "         [ 0.3722],\n",
       "         [ 0.5145],\n",
       "         [ 0.7045],\n",
       "         [ 0.7701],\n",
       "         [ 0.5966],\n",
       "         [ 0.4943],\n",
       "         [ 0.6354],\n",
       "         [ 0.6760],\n",
       "         [ 0.4017],\n",
       "         [ 0.4050],\n",
       "         [ 0.6181],\n",
       "         [ 0.4709],\n",
       "         [ 0.7746],\n",
       "         [ 0.5344],\n",
       "         [ 0.5759],\n",
       "         [ 0.3063],\n",
       "         [ 0.1567],\n",
       "         [ 0.2628],\n",
       "         [ 0.3366],\n",
       "         [ 0.4898],\n",
       "         [ 0.1306],\n",
       "         [ 0.3936],\n",
       "         [ 0.6012],\n",
       "         [ 0.5988],\n",
       "         [ 0.6238],\n",
       "         [ 0.5104],\n",
       "         [ 0.3937],\n",
       "         [ 0.6396],\n",
       "         [ 0.7066],\n",
       "         [ 0.6489],\n",
       "         [ 0.7095],\n",
       "         [ 0.8378],\n",
       "         [ 0.8534],\n",
       "         [ 0.7686],\n",
       "         [ 0.7123],\n",
       "         [ 0.7102],\n",
       "         [ 0.7432],\n",
       "         [ 0.6594],\n",
       "         [ 0.5104],\n",
       "         [ 0.6121]],\n",
       "\n",
       "        [[ 0.3035],\n",
       "         [ 0.1560],\n",
       "         [ 0.5010],\n",
       "         [ 0.9002],\n",
       "         [ 0.7201],\n",
       "         [ 0.5097],\n",
       "         [ 0.9140],\n",
       "         [ 0.2477],\n",
       "         [ 0.1288],\n",
       "         [ 0.6004],\n",
       "         [ 0.1823],\n",
       "         [ 0.4751],\n",
       "         [ 0.5562],\n",
       "         [ 0.5950],\n",
       "         [ 0.7845],\n",
       "         [ 0.6504],\n",
       "         [ 0.4192],\n",
       "         [ 0.4513],\n",
       "         [ 0.1617],\n",
       "         [ 0.1423],\n",
       "         [ 0.1094],\n",
       "         [ 0.2998],\n",
       "         [ 0.7755],\n",
       "         [ 0.5917],\n",
       "         [ 0.8769],\n",
       "         [ 0.7599],\n",
       "         [ 0.3376],\n",
       "         [ 0.1287],\n",
       "         [ 0.1718],\n",
       "         [ 0.4478],\n",
       "         [ 0.1005],\n",
       "         [ 0.7420],\n",
       "         [ 0.7533],\n",
       "         [ 0.2492],\n",
       "         [ 0.5183],\n",
       "         [ 0.4149],\n",
       "         [ 0.6134],\n",
       "         [ 0.8103],\n",
       "         [ 0.5166],\n",
       "         [ 0.5055],\n",
       "         [ 0.3227],\n",
       "         [ 0.2707],\n",
       "         [ 0.1877],\n",
       "         [ 0.5550],\n",
       "         [ 0.9001],\n",
       "         [ 0.7085],\n",
       "         [ 0.8903],\n",
       "         [ 0.6719],\n",
       "         [ 0.1271],\n",
       "         [ 0.3198],\n",
       "         [ 0.0420],\n",
       "         [ 0.1375],\n",
       "         [ 0.1949],\n",
       "         [ 0.0935],\n",
       "         [ 0.2311],\n",
       "         [ 0.1564],\n",
       "         [ 0.6334],\n",
       "         [ 0.5294],\n",
       "         [ 0.8075],\n",
       "         [ 0.6916],\n",
       "         [ 0.5214],\n",
       "         [ 0.2743],\n",
       "         [ 0.2660],\n",
       "         [ 0.1095],\n",
       "         [ 0.2872],\n",
       "         [ 0.7327],\n",
       "         [ 0.7209],\n",
       "         [ 0.7461],\n",
       "         [ 0.8110],\n",
       "         [ 0.5965],\n",
       "         [ 0.3374],\n",
       "         [ 0.8199],\n",
       "         [ 0.2421],\n",
       "         [ 0.1697],\n",
       "         [ 0.2187],\n",
       "         [ 0.6331],\n",
       "         [ 0.5013],\n",
       "         [ 0.7221],\n",
       "         [ 0.5814],\n",
       "         [ 0.6191],\n",
       "         [ 0.6331],\n",
       "         [ 0.7773],\n",
       "         [ 0.8030],\n",
       "         [ 0.5048],\n",
       "         [ 0.1929],\n",
       "         [ 0.2290],\n",
       "         [ 0.0681],\n",
       "         [ 0.0723],\n",
       "         [ 0.1036],\n",
       "         [ 0.2987],\n",
       "         [ 0.1716],\n",
       "         [ 0.4499],\n",
       "         [ 0.5684],\n",
       "         [ 0.3809],\n",
       "         [ 0.2634],\n",
       "         [ 0.4241],\n",
       "         [ 0.4765],\n",
       "         [ 0.6188],\n",
       "         [ 0.1264],\n",
       "         [ 0.1190],\n",
       "         [ 0.1609],\n",
       "         [ 0.1187],\n",
       "         [ 0.1933],\n",
       "         [ 0.8055],\n",
       "         [ 0.2983],\n",
       "         [ 0.6089],\n",
       "         [ 0.7059],\n",
       "         [ 0.3580],\n",
       "         [ 0.4830],\n",
       "         [ 0.6397],\n",
       "         [ 0.6743],\n",
       "         [ 0.8147],\n",
       "         [ 0.4758],\n",
       "         [ 0.5665],\n",
       "         [ 0.8128],\n",
       "         [ 0.8710],\n",
       "         [ 0.8539],\n",
       "         [ 0.8144],\n",
       "         [ 0.4690],\n",
       "         [ 0.8061],\n",
       "         [ 0.7551],\n",
       "         [ 0.4495],\n",
       "         [ 0.9753],\n",
       "         [ 0.7851],\n",
       "         [ 0.9302],\n",
       "         [ 0.9648],\n",
       "         [ 0.9196],\n",
       "         [ 0.8403],\n",
       "         [ 0.8829],\n",
       "         [ 0.8954],\n",
       "         [ 0.8011],\n",
       "         [ 0.6639],\n",
       "         [ 0.8076],\n",
       "         [ 0.7940],\n",
       "         [ 0.4463],\n",
       "         [ 0.3273],\n",
       "         [ 0.7771],\n",
       "         [ 0.2003],\n",
       "         [ 0.4076],\n",
       "         [ 0.4566],\n",
       "         [ 0.1741],\n",
       "         [ 0.2535],\n",
       "         [ 0.2871],\n",
       "         [ 0.2419],\n",
       "         [ 0.1371],\n",
       "         [ 0.3023],\n",
       "         [ 0.5245],\n",
       "         [ 0.2929],\n",
       "         [ 0.2382],\n",
       "         [ 0.1557],\n",
       "         [ 0.2481],\n",
       "         [ 0.3185],\n",
       "         [ 0.6412],\n",
       "         [ 0.2935],\n",
       "         [ 0.4841],\n",
       "         [ 0.3295],\n",
       "         [ 0.4240],\n",
       "         [ 0.5605],\n",
       "         [ 0.6849],\n",
       "         [ 0.6459],\n",
       "         [ 0.4271],\n",
       "         [ 0.7332],\n",
       "         [ 0.7920],\n",
       "         [ 0.9242],\n",
       "         [ 0.8366],\n",
       "         [ 0.8523],\n",
       "         [ 0.3937],\n",
       "         [ 0.1846],\n",
       "         [ 0.4855],\n",
       "         [ 0.3558],\n",
       "         [ 0.1429],\n",
       "         [ 0.3755],\n",
       "         [ 0.5314],\n",
       "         [ 0.7031],\n",
       "         [ 0.7879],\n",
       "         [ 0.8422],\n",
       "         [ 0.5824]],\n",
       "\n",
       "        [[ 0.1283],\n",
       "         [ 0.1218],\n",
       "         [ 0.1250],\n",
       "         [ 0.5001],\n",
       "         [ 0.1024],\n",
       "         [ 0.2286],\n",
       "         [ 0.5882],\n",
       "         [ 0.3200],\n",
       "         [ 0.0531],\n",
       "         [ 0.0767],\n",
       "         [ 0.0684],\n",
       "         [ 0.0755],\n",
       "         [ 0.2399],\n",
       "         [ 0.5515],\n",
       "         [ 0.1569],\n",
       "         [ 0.2575],\n",
       "         [ 0.2446],\n",
       "         [ 0.3361],\n",
       "         [ 0.4689],\n",
       "         [ 0.1418],\n",
       "         [ 0.0445],\n",
       "         [ 0.5089],\n",
       "         [ 0.9908],\n",
       "         [ 0.8310],\n",
       "         [ 0.8960],\n",
       "         [ 0.8115],\n",
       "         [ 0.7378],\n",
       "         [ 0.5673],\n",
       "         [ 0.7011],\n",
       "         [ 0.7248],\n",
       "         [ 0.4240],\n",
       "         [ 0.1135],\n",
       "         [ 0.1203],\n",
       "         [ 0.2863],\n",
       "         [ 0.1399],\n",
       "         [ 0.1468],\n",
       "         [ 0.0041],\n",
       "         [ 0.2110],\n",
       "         [ 0.0515],\n",
       "         [ 0.0811],\n",
       "         [ 0.0438],\n",
       "         [ 0.4663],\n",
       "         [ 0.0551],\n",
       "         [ 0.0574],\n",
       "         [ 0.8038],\n",
       "         [ 0.4904],\n",
       "         [ 0.4395],\n",
       "         [ 0.0512],\n",
       "         [ 0.1040],\n",
       "         [ 0.0911],\n",
       "         [ 0.3373],\n",
       "         [ 0.3478],\n",
       "         [ 0.4795],\n",
       "         [ 0.3590],\n",
       "         [ 0.1013],\n",
       "         [ 0.2331],\n",
       "         [ 0.0755],\n",
       "         [ 0.1825],\n",
       "         [ 0.3494],\n",
       "         [ 0.7404],\n",
       "         [ 0.0238],\n",
       "         [ 0.0143],\n",
       "         [ 0.1818],\n",
       "         [ 0.0773],\n",
       "         [ 0.1053],\n",
       "         [ 0.0718],\n",
       "         [ 0.2299],\n",
       "         [ 0.1390],\n",
       "         [ 0.0456],\n",
       "         [ 0.0740],\n",
       "         [ 0.2528],\n",
       "         [ 0.4224],\n",
       "         [ 0.9205],\n",
       "         [ 0.9778],\n",
       "         [ 1.0076],\n",
       "         [ 0.0570],\n",
       "         [ 0.0957],\n",
       "         [ 0.1987],\n",
       "         [ 0.0978],\n",
       "         [ 0.0893],\n",
       "         [ 0.0714],\n",
       "         [ 0.2055],\n",
       "         [ 0.1252],\n",
       "         [ 0.6143],\n",
       "         [ 0.1102],\n",
       "         [ 0.1880],\n",
       "         [ 0.0571],\n",
       "         [ 0.0287],\n",
       "         [ 0.2194],\n",
       "         [ 0.6102],\n",
       "         [ 0.5232],\n",
       "         [ 0.4280],\n",
       "         [ 0.8438],\n",
       "         [ 0.5960],\n",
       "         [ 0.1489],\n",
       "         [ 0.2407],\n",
       "         [ 0.1647],\n",
       "         [ 0.0613],\n",
       "         [ 0.0716],\n",
       "         [ 0.1520],\n",
       "         [ 0.1093],\n",
       "         [ 0.5112],\n",
       "         [ 0.1912],\n",
       "         [ 0.1550],\n",
       "         [ 0.1271],\n",
       "         [ 0.0514],\n",
       "         [ 0.1613],\n",
       "         [ 0.2961],\n",
       "         [ 0.3502],\n",
       "         [ 0.1997],\n",
       "         [ 0.0070],\n",
       "         [ 0.0665],\n",
       "         [ 0.0698],\n",
       "         [ 0.0781],\n",
       "         [ 0.0610],\n",
       "         [ 0.0253],\n",
       "         [ 0.0691],\n",
       "         [ 0.0071],\n",
       "         [-0.0020],\n",
       "         [ 0.0889],\n",
       "         [ 0.1070],\n",
       "         [ 0.7176],\n",
       "         [ 0.3741],\n",
       "         [ 0.2666],\n",
       "         [ 0.4023],\n",
       "         [ 0.9788],\n",
       "         [ 0.8189],\n",
       "         [ 0.8937],\n",
       "         [ 0.8864],\n",
       "         [ 0.8495],\n",
       "         [ 0.9772],\n",
       "         [ 0.8373],\n",
       "         [ 0.8386],\n",
       "         [ 0.5708],\n",
       "         [ 0.4586],\n",
       "         [ 0.2900],\n",
       "         [ 0.3205],\n",
       "         [ 0.4040],\n",
       "         [ 0.2356],\n",
       "         [ 0.1716],\n",
       "         [ 0.1582],\n",
       "         [ 0.3089],\n",
       "         [ 0.6367],\n",
       "         [ 0.4306],\n",
       "         [ 0.8292],\n",
       "         [ 0.4331],\n",
       "         [ 0.8344],\n",
       "         [ 0.6749],\n",
       "         [ 0.2437],\n",
       "         [ 0.0379],\n",
       "         [ 0.1223],\n",
       "         [ 0.1171],\n",
       "         [ 0.0785],\n",
       "         [ 0.0851],\n",
       "         [ 0.1070],\n",
       "         [ 0.1275],\n",
       "         [ 0.2660],\n",
       "         [ 0.0470],\n",
       "         [ 0.3132],\n",
       "         [ 0.4826],\n",
       "         [ 0.2329],\n",
       "         [ 0.4280],\n",
       "         [ 0.3022],\n",
       "         [ 0.8835],\n",
       "         [ 0.1318],\n",
       "         [ 0.3365],\n",
       "         [ 0.4515],\n",
       "         [ 0.0375],\n",
       "         [ 0.1610],\n",
       "         [ 0.1993],\n",
       "         [ 0.1060],\n",
       "         [ 0.1443],\n",
       "         [ 0.2000],\n",
       "         [ 0.0052],\n",
       "         [ 0.1745],\n",
       "         [ 0.5124],\n",
       "         [ 0.0677]]], device='mps:0', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.prediction['dms'][batch.data['dms'][\"index\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = dm.find_one_index_per_data_type('valid')\n",
    "print(indexes)\n",
    "for data, metadata in dm.val_dataloader():\n",
    "    print(metadata['index'])\n",
    "    for data_type in ['dms', 'shape']:\n",
    "        if indexes[data_type] in set(metadata['index']):\n",
    "            idx = metadata['index'].index(indexes[data_type])\n",
    "            batch_idx = data[data_type]['index'][idx].item()\n",
    "            print(idx, batch_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild this into a series of lines\n",
    "\n",
    "REFERENCE_METRIC = {\n",
    "    'structure': 'f1', \n",
    "    'dms': 'mae',\n",
    "    'shape': 'mae'\n",
    "}\n",
    "\n",
    "from dmsensei.core.metrics import metric_factory\n",
    "from torch import tensor\n",
    "\n",
    "lines = []\n",
    "for a in dm.predict_dataloader():\n",
    "    break\n",
    "data, metadata = a\n",
    "\n",
    "for idx in range(len(metadata['index'])):\n",
    "    line = {}\n",
    "    for k, v in metadata.items():\n",
    "        line[k] = v[idx]\n",
    "    # for k, v in predictions.items():\n",
    "    #     line['pred_{}'.format(k)] = v[idx]\n",
    "    lines.append(line)\n",
    "for data_type, vals in data.items():\n",
    "    for k,v in zip(vals['index'], vals['values'].tolist()):\n",
    "        name = \"true_{}\".format(data_type) if data_type != 'sequence' else 'sequence'\n",
    "        lines[k.item()][name] = v\n",
    "        lines[k.item()][name.replace('true', 'pred')] = v\n",
    "        \n",
    "for data_type in ['dms','shape']:\n",
    "    for line in lines:\n",
    "        if not ('true_{}'.format(data_type) in line and 'pred_{}'.format(data_type) in line):\n",
    "            continue\n",
    "        line['score_{}'.format(data_type)] = metric_factory[REFERENCE_METRIC[data_type]](pred=tensor(line['pred_{}'.format(data_type)]), true=tensor(line['true_{}'.format(data_type)]), batch=False)\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, metadata = a\n",
    "# rebuild this into a series of lines\n",
    "df_lines = pd.DataFrame(metadata)\n",
    "for data_type, arr in data.items():\n",
    "    df_lines = df_lines.merge(\n",
    "        pd.DataFrame(\n",
    "             pd.Series(arr['values'].tolist(), index=arr['index'].tolist()), columns=[data_type]),\n",
    "         how='outer', left_index=True, right_index=True)\n",
    "df_lines    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import json\n",
    "\n",
    "json.dump(lines, open('t.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.DataFrame(a[1])\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0]['dms']['values'].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(a[0]['dms']['values'].tolist(), index=a[0]['dms']['index'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = trainer.predict(\n",
    "        model,\n",
    "        datamodule=DataModule(\n",
    "            name=[\"ribo-test\"],\n",
    "            data='sequence',\n",
    "            force_download=False,\n",
    "            batch_size=256,\n",
    "            num_workers=1,\n",
    "            train_split=0,\n",
    "            valid_split=0,\n",
    "            predict_split=1.,\n",
    "            overfit_mode=False,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format the prediction and save it locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def format_to_ribonanza(prediction):\n",
    "    \n",
    "    # load data (EDIT THIS)\n",
    "    data = json.load(open('/root/DMSensei/scripts/data/input_files/ribo-test/data.json'))\n",
    "    \n",
    "    # reformat into individual sequences\n",
    "    arr = [p for batch in prediction for p in batch]\n",
    "    \n",
    "    # remove padding\n",
    "    arr = [a[:s] for a,s in zip(arr, [len(d['sequence']) for d in data.values()])] \n",
    "    \n",
    "    # stack into dataframe\n",
    "    prediction = np.vstack(arr)\n",
    "    return pd.DataFrame(prediction, columns=[\"reactivity_DMS_MaP\", \"reactivity_2A3_MaP\"]).reset_index().rename(columns={\"index\": \"id\"})\n",
    "\n",
    "format_to_ribonanza(prediction).to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push the prediction to Kaggle\n",
    "\n",
    "Note: setup your Kaggle authentification first:\n",
    "1. download your Kaggle API keys `kaggle.json` here: https://www.kaggle.com/settings/account\n",
    "2. save it to `~/.kaggle/kaggle.json`:\n",
    "\n",
    "    ```bash\n",
    "    mv ~/Downloads/kaggle.json ~/.kaggle/kaggle.json\n",
    "    ```\n",
    "3. push your results to kaggle using the following line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('kaggle competitions submit -c stanford-ribonanza-rna-folding -f submission.csv -m \"test commit\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
