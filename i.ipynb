{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ribo-HQ: Loading data.pkl...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "from dmsensei import DataModule, create_model\n",
    "from dmsensei.config import device\n",
    "from dmsensei.core.callbacks import WandbFitLogger, KaggleLogger, WandbTestLogger\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "from lightning.pytorch import Trainer\n",
    "from dmsensei.config import device\n",
    "import sys\n",
    "import os\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "# Create dataset\n",
    "dm = DataModule(\n",
    "    name=[\"ribo-HQ\"],\n",
    "    data_type=[\"dms\", \"shape\"],\n",
    "    force_download=False,\n",
    "    batch_size=128,\n",
    "    num_workers=0,\n",
    "    train_split=512,\n",
    "    valid_split=0,\n",
    "    predict_split=0,\n",
    "    overfit_mode=True,\n",
    "    shuffle_valid=False,\n",
    ")\n",
    "\n",
    "dm.setup('fit')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = create_model(\n",
    "    model=\"evoformer\",\n",
    "    data=\"multi\",\n",
    "    quality=True,\n",
    "    ntoken=5,\n",
    "    d_model=64,\n",
    "    c_z=8,\n",
    "    num_blocks=4,\n",
    "    no_recycles=0,\n",
    "    dropout=0,\n",
    "    lr=3e-3,\n",
    "    weight_decay=0,\n",
    "    gamma=0.997,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=200,\n",
    "    callbacks=[\n",
    "        WandbFitLogger(dm=dm, batch_size=128, log_every_n_epoch=1),\n",
    "        WandbTestLogger(dm=dm),\n",
    "        # LearningRateMonitor(logging_interval=\"step\"),\n",
    "    ],\n",
    "    logger=WandbLogger(project=\"dmsensei\", log_model=True),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, wandb\n",
    "import numpy as np\n",
    "from torch.nn.functional import mse_loss\n",
    "# wandb.init(project=\"dmsensei\", name=\"debug-transformer\", tags=[\"debug\", \"transformer\"])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "for epoch in range(1000):\n",
    "    losses = []\n",
    "    maes = []\n",
    "    for batch in dm.train_dataloader():\n",
    "        pred = model(batch.get('sequence'))\n",
    "        dms = pred['dms'][batch.get_index('dms')], batch.get('dms')\n",
    "        shape = pred['shape'][batch.get_index('shape')], batch.get('shape')\n",
    "        true = torch.cat([dms[1], shape[1]], dim=0)\n",
    "        pred = torch.cat([dms[0], shape[0]], dim=0)\n",
    "        mask = true!=-1000.\n",
    "        loss = mse_loss(pred[mask], true[mask])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        maes.append((pred[mask]-true[mask]).abs().mean().item())\n",
    "    print(f\"Epoch {epoch} loss: {np.sqrt(np.mean(losses)).round(3)} mae: {np.mean(maes).round(3)} \\r\", end=\"\")\n",
    "    # wandb.log({\"loss\": np.sqrt(np.mean(losses)).round(3), \"epoch\": epoch, \"mae\": np.mean(maes).round(3)})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
