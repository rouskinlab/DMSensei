{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from disk...\n",
      "Load structure               \r"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "from dmsensei import DataModule, create_model\n",
    "from dmsensei.config import device\n",
    "from dmsensei.core.callbacks import WandbFitLogger, KaggleLogger, WandbTestLogger\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "from lightning.pytorch import Trainer\n",
    "from dmsensei.config import device\n",
    "import sys\n",
    "import os\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import torch, wandb\n",
    "import numpy as np\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "# Create dataset\n",
    "dm = DataModule(\n",
    "    name=[\"ribo-kaggle\"],\n",
    "    data_type=[\"dms\", \"shape\",'structure'],\n",
    "    force_download=False,\n",
    "    batch_size=32,\n",
    "    num_workers=0,\n",
    "    train_split=256,\n",
    "    valid_split=0,\n",
    "    predict_split=0,\n",
    "    overfit_mode=True,\n",
    "    shuffle_valid=False,\n",
    ")\n",
    "\n",
    "dm.setup('fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/DMSensei/dmsensei/models/transformer.py:108: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(m.weight * 0.001)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "lr = 1E-3\n",
    "model = create_model(\n",
    "    model=\"transformer\",\n",
    "    # weight_data=True,\n",
    "    ntoken=5,\n",
    "    d_model=16,\n",
    "    nhead=16,\n",
    "    d_hid=13,\n",
    "    nlayers=2,\n",
    "    dropout=0.0,\n",
    "    lr=lr,\n",
    "    weight_decay=0,\n",
    "    gamma=0.997,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ltr7akn1) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d3ce8e124b4406b6bd845c50c69d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/loss</td><td>▇█▇▇▇▇▇▆▆▆▆▇▅▆▆▅▅▄▅▅▄▅▄▄▃▄▃▄▄▃▂▂▂▂▁▂▂▂▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>train/loss</td><td>0.27619</td></tr><tr><td>trainer/global_step</td><td>1599</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">debug-transformer-fw-logloss-lr:0.001-dms_shape</strong> at: <a href='https://wandb.ai/rouskin-lab/DMSensei/runs/ltr7akn1' target=\"_blank\">https://wandb.ai/rouskin-lab/DMSensei/runs/ltr7akn1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231129_230202-ltr7akn1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ltr7akn1). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb13d55202f94b8d86cec8a8c7ab6644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113352488933338, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/DMSensei/wandb/run-20231129_230518-i9ymuk84</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rouskin-lab/DMSensei/runs/i9ymuk84' target=\"_blank\">small_tra-mseloss-lr:0.001</a></strong> to <a href='https://wandb.ai/rouskin-lab/DMSensei' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rouskin-lab/DMSensei' target=\"_blank\">https://wandb.ai/rouskin-lab/DMSensei</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rouskin-lab/DMSensei/runs/i9ymuk84' target=\"_blank\">https://wandb.ai/rouskin-lab/DMSensei/runs/i9ymuk84</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/root/.pyenv/versions/3.11.6/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type               | Params\n",
      "-----------------------------------------------------------\n",
      "0 | loss_signal_fn      | MSELoss            | 0     \n",
      "1 | encoder             | Embedding          | 80    \n",
      "2 | pos_encoder         | PositionalEncoding | 0     \n",
      "3 | transformer_encoder | Sequential         | 3.2 K \n",
      "4 | resnet              | Sequential         | 9.7 K \n",
      "5 | output_net_DMS      | Sequential         | 1.2 K \n",
      "6 | output_net_SHAPE    | Sequential         | 1.2 K \n",
      "7 | output_net          | Linear             | 34    \n",
      "8 | drop                | Dropout            | 0     \n",
      "-----------------------------------------------------------\n",
      "15.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.4 K    Total params\n",
      "0.062     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bfa8b02f6854d8d85d3c7b2a188b6f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.11.6/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=27` in the `DataLoader` to improve performance.\n",
      "/root/.pyenv/versions/3.11.6/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:104: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "/root/.pyenv/versions/3.11.6/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=27` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e903b2c509274084aca15db0649c6727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.11.6/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"dmsensei\", name=\"small_tranfo-mseloss-lr:{}\".format(lr))\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=1000,\n",
    "    log_every_n_steps=1,\n",
    "    callbacks=[\n",
    "        WandbFitLogger(dm=dm, batch_size=dm.batch_size, log_plots_every_n_epoch=100000),\n",
    "    ],\n",
    "    logger=WandbLogger(),\n",
    ")\n",
    "\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "import json\n",
    "\n",
    "data = json.load(open('/root/DMSensei/data/datafolders/ribo-kaggle/data.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_data = {}\n",
    "for idx, (key, val) in enumerate(data.items()):\n",
    "    if idx > 100:\n",
    "        break\n",
    "    mini_data[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dmsensei import DataModule, create_model\n",
    "from dmsensei.config import device\n",
    "from dmsensei.core.callbacks import WandbFitLogger, KaggleLogger, WandbTestLogger\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "from lightning.pytorch import Trainer\n",
    "from dmsensei.config import device\n",
    "import sys\n",
    "import os\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import torch, wandb\n",
    "import numpy as np\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from typing import List\n",
    "from dmsensei.core.batch import Batch\n",
    "from dmsensei.huggingface import get_dataset\n",
    "from dmsensei.config import device\n",
    "from dmsensei.core.datatype import DMSDataset, SHAPEDataset, StructureDataset\n",
    "import torch.nn.functional as F\n",
    "from dmsensei.core.embeddings import sequence_to_int\n",
    "from dmsensei.core.util import _pad\n",
    "\n",
    "class Dataset(TorchDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        data_type: List[str],\n",
    "        force_download: bool,\n",
    "        tqdm=True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.data_type = data_type\n",
    "        data = get_dataset(\n",
    "            name=name,\n",
    "            force_download=force_download,\n",
    "            tqdm=tqdm,\n",
    "        )\n",
    "        self.length = [len(d['sequence']) for d in data.values()]\n",
    "        self.refs = list(data.keys())\n",
    "        self.L = max(self.length)\n",
    "        self.sequence = torch.stack([_pad(sequence_to_int(data[ref]['sequence']), self.L, 'sequence') for ref in self.refs])\n",
    "        self.dms, self.shape, self.structure = None, None, None\n",
    "        if 'dms' in data_type:\n",
    "            self.dms = DMSDataset.from_data_json(data, self.L, self.refs)\n",
    "        if 'shape' in data_type:\n",
    "            self.shape = SHAPEDataset.from_data_json(data, self.L, self.refs)\n",
    "        if 'structure' in data_type:\n",
    "            self.structure = StructureDataset.from_data_json(data, self.L, self.refs)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.sequence)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple:\n",
    "        return {\n",
    "            'reference': self.refs[index],\n",
    "            'sequence': self.sequence[index],\n",
    "            'length': self.length[index],\n",
    "            'dms': self.dms[index] if self.dms is not None else None,\n",
    "            'shape': self.shape[index] if self.shape is not None else None,\n",
    "            'structure': self.structure[index] if self.structure is not None else None,\n",
    "        }\n",
    "\n",
    "    def collate_fn(self, batch_data):\n",
    "        batch = Batch.from_dataset_items(batch_data, self.data_type)\n",
    "        return batch\n",
    "\n",
    "            \n",
    "dataset = Dataset(\n",
    "    name = 'test',\n",
    "    data_type = ['dms', 'shape', 'structure'],\n",
    "    force_download=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dmsensei.core.datatype import DataType\n",
    "from dmsensei.core.datatype import data_type_factory\n",
    "import torch\n",
    "\n",
    "def split_data_type(data_type):\n",
    "    if not \"_\" in data_type:\n",
    "        data_part = \"true\"\n",
    "    else:\n",
    "        data_part, data_type = data_type.split(\"_\")\n",
    "    return data_part, data_type\n",
    "\n",
    "\n",
    "class Batch:\n",
    "    def __init__(\n",
    "        self,\n",
    "        reference,\n",
    "        sequence,\n",
    "        length,\n",
    "        L,\n",
    "        batch_size,\n",
    "        data_types,\n",
    "        dms=None,\n",
    "        shape=None,\n",
    "        structure=None,\n",
    "    ):\n",
    "        self.reference = reference\n",
    "        self.sequence = sequence\n",
    "        self.length = length\n",
    "        self.dms = dms\n",
    "        self.shape = shape\n",
    "        self.structure = structure\n",
    "        self.L = L\n",
    "        self.batch_size = batch_size\n",
    "        self.data_types = data_types\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset_items(cls, datapoints: list, data_type: str):\n",
    "        reference= [d['reference'] for d in datapoints]\n",
    "        length = [d['length'] for d in datapoints]\n",
    "        L = max(length)\n",
    "        sequence= torch.stack([d['sequence'][:L] for d in datapoints])\n",
    "        batch_size = len(datapoints)\n",
    "        \n",
    "        data = {}\n",
    "        for dt in data_type:\n",
    "            true, error, index = [], [], []\n",
    "            for idx, dp in enumerate(datapoints):\n",
    "                if dt in dp and dp[dt] is not None:\n",
    "                    true.append(dp[dt]['true'])\n",
    "                    if dt != 'structure':\n",
    "                        error.append(dp[dt]['error']) \n",
    "                    index.append(idx)\n",
    "            \n",
    "            if len(true) == 0:\n",
    "                continue\n",
    "            \n",
    "\n",
    "            if dt != 'structure':\n",
    "                data[dt] = data_type_factory['batch'][dt](\n",
    "                    true=torch.stack(true),\n",
    "                    pred=None,\n",
    "                    error=torch.stack(error),\n",
    "                    index=torch.tensor(index),\n",
    "                )\n",
    "            else:\n",
    "                data[dt] = data_type_factory['batch'][dt](\n",
    "                    true=torch.stack(true),\n",
    "                    pred=None,\n",
    "                    index=torch.tensor(index),\n",
    "                )\n",
    "            \n",
    "        return cls(\n",
    "            reference=reference,\n",
    "            sequence=sequence,\n",
    "            length=length,\n",
    "            L=L,\n",
    "            batch_size=batch_size,\n",
    "            data_types=data_type,\n",
    "            **data,\n",
    "        )\n",
    "        \n",
    "\n",
    "    def get(self, data_type, index=None, to_numpy=False):\n",
    "        if not self.contains(data_type):\n",
    "            raise ValueError(f\"Batch does not contain {data_type}\")\n",
    "        \n",
    "        if data_type in [\"reference\", \"sequence\", \"length\"]:\n",
    "            out = getattr(self, data_type)\n",
    "        else:\n",
    "            data_part, data_type = split_data_type(data_type)\n",
    "            out = getattr(getattr(self, data_type), data_part)\n",
    "        \n",
    "        if index is not None:\n",
    "            out = out[index]\n",
    "            if hasattr(out, '__len__'):\n",
    "                out = out[:self.get(\"length\")[index]]\n",
    "\n",
    "        if to_numpy:\n",
    "            if hasattr(out, \"cpu\"):\n",
    "                out = out.squeeze().cpu().numpy()\n",
    "        return out\n",
    "        \n",
    "batch = dataset.collate_fn([dataset[0], dataset[1], dataset[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.get('sequence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from dmsensei.core.dataset import Dataset\n",
    "from dmsensei.core.batch import Batch\n",
    "\n",
    "dataset = Dataset(\n",
    "    name = 'test',\n",
    "    data_type = ['dms', 'shape', 'structure'],\n",
    "    force_download=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Batch.from_dataset_items([dataset[0], dataset[1], dataset[2]], ['dms', 'shape', 'structure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.get('error_shape')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
